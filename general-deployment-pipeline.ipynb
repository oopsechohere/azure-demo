{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, Environment, Experiment, Datastore, Dataset, ScriptRunConfig\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute, DatabricksCompute\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "from azureml.pipeline.steps import HyperDriveStep, HyperDriveStepRun, PythonScriptStep, DatabricksStep\n",
    "from azureml.pipeline.core import Pipeline, PipelineData, TrainingOutput\n",
    "from azureml.train.hyperdrive import RandomParameterSampling, BanditPolicy, HyperDriveConfig, PrimaryMetricGoal\n",
    "from azureml.train.hyperdrive import choice, loguniform\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.core import PipelineParameter\n",
    "from azureml.pipeline.core import Pipeline, StepSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up the compute of the ML pipeline, which should be compute cluster\n",
    "ws = Workspace.from_config()\n",
    "cluster_name = \"samplename\"\n",
    " \n",
    "compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up the datastore that you will use for input data\n",
    "def_blob_store = ws.get_default_datastore()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
