{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, Environment, Experiment, Datastore, Dataset, ScriptRunConfig\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute, DatabricksCompute\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "from azureml.pipeline.steps import HyperDriveStep, HyperDriveStepRun, PythonScriptStep, DatabricksStep\n",
    "from azureml.pipeline.core import Pipeline, PipelineData, TrainingOutput\n",
    "from azureml.train.hyperdrive import RandomParameterSampling, BanditPolicy, HyperDriveConfig, PrimaryMetricGoal\n",
    "from azureml.train.hyperdrive import choice, loguniform\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.core import PipelineParameter\n",
    "from azureml.pipeline.core import Pipeline, StepSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up the compute of the ML pipeline, which should be compute cluster\n",
    "ws = Workspace.from_config()\n",
    "cluster_name = \"samplename\"\n",
    " \n",
    "compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up the datastore that you will use for input data\n",
    "def_blob_store = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an appropriate Azure cloud ML environment, or you could also customized your own customized environment based on personal needs\n",
    "env = Environment.get(ws, name=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the RunConfiguration object, it is necessary for submitting any runs in an experiment\n",
    "rcfg = RunConfiguration()\n",
    "rcfg.environment = env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build customized Pipeline Steps:\n",
    "a. Now we are creating some pipeline steps in sequence for my demo pipeline. There are many ways to create each pipeline step, and you can design as many steps as you need with any sequences for your ML pipeline. For this demo pipeline, I will only create 2 pipeline steps. by using the PythonScriptStep. The first pipeline step is called \"Data_Clean\", and the subsequent step is called \"Model_Train\" \n",
    "b. Build the first pipeline step: Data_Clean. The purpose of this step is for data cleansing (pipeline parameter):\n",
    "Details of PIpelineparameter Class:https://learn.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.graph.pipelineparameter?view=azure-ml-py \n",
    "\n",
    "Make sure that the datapath URI actually exits in the datastore which you have setup connection with the data container, and make sure is is the type of Datasotre URI for the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.datastore import Datastore\n",
    "from azureml.data.datapath import DataPath, DataPathComputeBinding\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core import PipelineParameter\n",
    " \n",
    "datapath = 'Datastore URI1'\n",
    "datapath2 = 'Datastore URI2'\n",
    "data_path_pipeline_param = PipelineParameter(name=\"input_data_train\", default_value=datapath)\n",
    "data_path_pipeline_param2 = PipelineParameter(name=\"input_data_predict\", default_value=datapath2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up output data destinations. For this demo, I setup 2 output data configuration. One is for the cleaned training data , and the other one is for the cleaned input data that we need to make prediction later\n",
    "from azureml.data import OutputFileDatasetConfig\n",
    " \n",
    "output_train = OutputFileDatasetConfig(name=\"demo1_processed_train\",\n",
    "                                 destination=(def_blob_store, \"\")).read_delimited_files().register_on_complete('demo1_processed_train')\n",
    " \n",
    "output_inputdata = OutputFileDatasetConfig(name=\"demo1_processed_inputdata\",\n",
    "                                 destination=(def_blob_store, \"\")).read_delimited_files().register_on_complete('demo1_processed_inputdata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline step1 to trigger the 'data_cleaning3.py' script. For the 'data_cleaning.py' script: data_cleaning.py\n",
    "data_clean = PythonScriptStep(script_name='data_cleaning.py',\n",
    "                                       source_directory=script_folder,\n",
    "                                       name=\"Data_Clean\",\n",
    "                                       compute_target=compute_target,\n",
    "                                       arguments=['--input_data_path_train', data_path_pipeline_param,\n",
    "                                                  '--input_data_path_predict', data_path_pipeline_param2,\n",
    "                                                  '--processed_data_train', output_train,\n",
    "                                                  '--processed_data_inputdata', output_inputdata],\n",
    "                                    #    inputs=[datapath_input],\n",
    "                                       allow_reuse=True,\n",
    "                                       runconfig=rcfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the second pipeline step: model_train. The purpose of this step to train the model with registering the model to AML portal, and produce the prediction output to the target azure data container. \n",
    "model_train = PythonScriptStep(script_name='model_train.py',\n",
    "                                      source_directory=script_folder,\n",
    "                                      name=\"Model_Train\",\n",
    "                                      compute_target=compute_target,\n",
    "                                      arguments=['--cleaned_data_train', output_train.as_input('cleaned_data_train'),\n",
    "                                                 '--cleaned_data_predict', output_inputdata.as_input('cleaned_data_predict')],\n",
    "                                   #    inputs=[datapath_input],\n",
    "                                      allow_reuse=True,\n",
    "                                      runconfig=rcfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[Input_Data, Data_Clean, Model_train, Register_Model, Data_output])\n",
    "pipeline_run = exp.submit(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run = exp.submit(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Publish pipeline to an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineEndpoint\n",
    " \n",
    "pipeline_name = \"Demo_Endpoint\"\n",
    " \n",
    "if [x for x in PipelineEndpoint.list(ws) if x.name == pipeline_name]:\n",
    "    pipeline_endpoint = PipelineEndpoint.get(ws, name=pipeline_name)\n",
    "    pipeline_endpoint.add_default(published_pipeline)\n",
    "else:\n",
    "    pipeline_endpoint = PipelineEndpoint.publish(workspace=ws, name=pipeline_name,\n",
    "                                             pipeline=published_pipeline, description=\"....\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deployment Step 2: Setup a scheduler on Synapse for the ML pipeline endpoint\n",
    "Linked the ML service on Synapse, and trigger it with a scheduler "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
